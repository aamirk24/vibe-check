Vibe-Check VibeCheck is an intelligent web application that captures live webcam footage, analyzes the user's facial expression to determine their mood, and then suggests a curated Spotify playlist to match that vibe. This project demonstrates a complete MLOps (Machine Learning Operations) feedback loop, where user feedback is collected and used to fine-tune the underlying AI model for improved accuracy over time.üöÄ Key FeaturesLive Mood Analysis: Utilizes a live webcam feed and a deep learning model to perform real-time facial emotion recognition.Dynamic Spotify Playlists: Connects to the Spotify API to search for and embed playlists that match the detected mood (e.g., "happy vibes", "sad songs").MLOps Feedback Loop: Users can provide corrective feedback if the detected mood is incorrect. This feedback is logged to Comet ML.Model Fine-Tuning: A dedicated script downloads the user feedback and images from Comet ML to fine-tune the AI model, creating a new, more accurate version.Confidence Score: The UI displays a confidence bar, showing how certain the model is about its prediction.Modern UI: A sleek, responsive, and dark-mode interface for an excellent user experience.‚ú® Innovative Use of Comet ML for Production ObservabilityThis project leverages Comet not just as an experiment tracker for training, but as a critical production monitoring and observability tool. This approach provides deep insights into the model's real-world performance.Traceability from Input to Feedback:Every analysis generates a unique prediction_id.This ID is used to name the input image asset (input_{id}.jpg) and is also passed to the frontend.When a user submits feedback, this same prediction_id is included in the feedback log.Innovation: This creates an unbreakable, traceable link between a specific input image, the model's output, and the user's "ground truth" label. This allows for precise debugging and analysis of individual prediction failures.Live Model Observability:By logging every prediction's input image, output label (mood_prediction), and confidence score, we gain a live dashboard of our model's behavior.Innovation: We can visually inspect the exact images that cause low-confidence predictions or are frequently misclassified, helping us identify patterns (e.g., the model struggles with poor lighting, certain angles, or specific expressions).Real-World Evaluation Metrics:The feedback_log.csv generated from user feedback is more than just a log; it's a dataset for calculating real-world accuracy.Innovation: Unlike evaluation on a static test set, this allows us to answer crucial business-facing questions like, "What was our model's actual accuracy with users this week?" or "Which two emotions are most often confused?" This measures true performance and guides the decision of when it's time to retrain the model.This system transforms Comet from a simple training utility into the backbone of a data-centric MLOps workflow, enabling continuous improvement driven directly by user interaction.üõ†Ô∏è Tech StackBackend: Python, FlaskFrontend: HTML5, CSS3, JavaScriptML Model: Hugging Face Transformers (dima806/facial_emotions_image_detection)MLOps & Experiment Tracking: Comet MLMusic: Spotify API (via spotipy library)Core ML Libraries: PyTorch, Pillow, datasets, evaluate, accelerateüìÅ Project StructureThe project is organized into a modular structure for clarity and maintainability.moody_playlist/
|
|-- app.py                  # Main Flask application, handles routes and API calls.
|-- mood_detector.py        # Loads the local AI model and performs mood analysis.
|-- spotify_player.py       # Handles interaction with the Spotify API.
|-- download_data.py        # Script to download feedback data from Comet ML.
|-- finetune.py             # Script to fine-tune the AI model with new data.
|
|-- templates/
|   |-- index.html          # The main HTML structure for the frontend.
|
|-- static/
|   |-- css/
|   |   |-- style.css       # All styling rules for the application.
|   |-- js/
|       |-- script.js       # Client-side logic for webcam, UI, and API calls.
|
|-- finetuning_dataset/     # (Generated by download_data.py) Holds images for training.
|-- vibecheck-mood-detector-v2/ # (Generated by finetune.py) The new, improved model.
|
|-- .env                    # (Crucial) Stores all secret API keys.
|-- requirements.txt        # Lists all Python dependencies.
|-- README.md               # This file.
‚öôÔ∏è Setup and InstallationFollow these steps to set up the project locally.1. Clone the Repositorygit clone <your-repository-url>
cd moody_playlist
2. Create and Activate a Virtual Environment# For Unix/macOS
python3 -m venv venv
source venv/bin/activate

# For Windows
python -m venv venv
.\venv\Scripts\activate
3. Install DependenciesInstall all required Python packages from the requirements.txt file.pip install -r requirements.txt
Note: The finetune.py script requires additional libraries (datasets, accelerate, etc.), which can be installed separately when needed.4. Configure API KeysThis is the most critical step. Create a file named .env in the root of the project directory and add your API keys.# .env file

# Hugging Face (optional, but good practice)
HUGGINGFACE_API_KEY="your_hugging_face_api_key"

# Spotify API Credentials
SPOTIPY_CLIENT_ID="your_spotify_client_id"
SPOTIPY_CLIENT_SECRET="your_spotify_client_secret"

# Comet ML Credentials
COMET_API_KEY="your_comet_api_key"
COMET_WORKSPACE="your_comet_workspace_name"
COMET_PROJECT_NAME="moody-playlist"
‚ñ∂Ô∏è How to Run: The Complete MLOps WorkflowThis project is designed to be run in a cycle. Follow these steps from start to finish.Step 1: Run the App & Collect DataInitially, run the application to gather feedback data.Ensure mood_detector.py uses the base model:# in mood_detector.py
MODEL_PATH = "dima806/facial_emotions_image_detection"
Start the Flask server:python app.py
Use the App: Open your browser to http://127.0.0.1:5000. Use the "Find My Vibe" feature multiple times and, most importantly, submit feedback for each analysis. This logs the data to Comet ML.Step 2: Download Data for TrainingOnce you have collected sufficient feedback (e.g., 20+ examples), stop the Flask server (CTRL+C) and run the download script.python download_data.py
This will create the finetuning_dataset folder, containing your images sorted by their correct mood labels.Step 3: Fine-Tune the ModelNow, train a new version of the model on your custom dataset. This process can be resource-intensive.Install extra dependencies if you haven't already:pip install datasets accelerate evaluate scikit-learn torchvision
Run the fine-tuning script:python finetune.py
This creates the vibecheck-mood-detector-v2 folder, which contains your new, improved model.Step 4: Run the App with Your Improved ModelFinally, point the application to your new local model.Update mood_detector.py:# in mood_detector.py
MODEL_PATH = "./vibecheck-mood-detector-v2"
Restart the Flask server:python app.py
Your VibeCheck application is now running with a more accurate model that has been custom-trained on your own feedback! You can repeat this cycle as you collect more data to continuously improve its performance.
